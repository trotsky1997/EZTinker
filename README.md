# EZTinker

A **minimal Tinker** clone for distributed model training - **ç”¨æˆ·åœ¨æœ¬åœ°å†™è®­ç»ƒå¾ªç¯/ç®—æ³•ï¼ŒæœåŠ¡ç«¯è´Ÿè´£æŠŠæ“ä½œå¯é åœ°è·‘åœ¨ GPU é›†ç¾¤ä¸Š**ã€‚

## MVP æ ¸å¿ƒåŠŸèƒ½

âœ¨ **å››æ ¸å¿ƒ API åŸè¯­**ï¼ˆå®Œå…¨å¯¹é½ Tinker è®¾è®¡ï¼‰:
- `forward_backward()`: å‰å‘ + åå‘ã€æ¢¯åº¦ç´¯ç§¯
- `optim_step()`: å‚æ•°æ›´æ–°
- `sample()`: æ¨ç†é‡‡æ ·
- `save_state()` / `load_state()`: ä¿å­˜/åŠ è½½ checkpoints

âœ¨ **LoRA é€‚é…è®­ç»ƒ**: ï¼ˆèŠ‚çœæˆæœ¬ã€å¿«é€Ÿè¿­ä»£ï¼‰
- Base model åªè¯»åŠ è½½
- LoRA adapter å¯è®­ç»ƒ
- æ”¯æŒ checkpoint æ–­ç‚¹ç»­è®­

âœ¨ **Job/Future æ¨¡å¼**:
- å¼‚æ­¥æäº¤è®­ç»ƒä»»åŠ¡
- è½®è¯¢è·å–ç»“æœ
- å¯é çš„å¼‚æ­¥æ‰§è¡Œ

## æ¶æ„è®¾è®¡

```
EZTinker Service
â”œâ”€â”€ ğŸš€ API Layer (FastAPI)
â”œâ”€â”€ ğŸ§  Training Engine (PyTorch + LoRA)
â”œâ”€â”€ ğŸ”® Sampling Engine (Inference)
â”œâ”€â”€ ğŸ’¾ Checkpoint Manager
â””â”€â”€ ğŸ–¥ï¸  CLI (Typer)

æ ¸å¿ƒæ•°æ®æµ:
Client <--HTTP--> API <--State--> TrainingRun <--LoRA--> Model
                               â””--Sampler--> Inference
```

## å®‰è£…

```bash
# ä½¿ç”¨ uv åˆ›å»ºé¡¹ç›®
uv init --lib eztinker

# å®‰è£…ä¾èµ–
uv add fastapi uvicorn typer pydantic torch transformers peft accelerate redis

# å®‰è£… Ruff
uv add --dev ruff
```

## å¿«é€Ÿå¼€å§‹

### 1. å¯åŠ¨ EZTinker æœåŠ¡

```bash
uv run eztinker server
# æˆ–åœ¨å¼€å‘æ¨¡å¼ï¼ˆè‡ªåŠ¨é‡è½½ï¼‰
uv run --reload eztinker server --reload
```

æœåŠ¡å¯åŠ¨åœ¨ `http://localhost:8000`ï¼Œä½ å¯ä»¥ï¼š
- API docs: `http://localhost:8000/docs`
- Health check: `http://localhost:8000/health`

### 2. åˆ›å»ºè®­ç»ƒä¼šè¯

```python
import requests

# åˆ›å»ºåŸºäº GPT-2 çš„è®­ç»ƒä¼šè¯
response = requests.post(
    "http://localhost:8000/v1/runs",
    json={
        "base_model": "gpt2",
    }
)
run_id = response.json()["run_id"]
print(f"Training run created: {run_id}")
```

### 3. æ‰§è¡Œè®­ç»ƒå¾ªç¯

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("gpt2")

# å‡†å¤‡æ•°æ®
text = "This is a training example"
batch = tokenizer(text, return_tensors="pt")

# Tinker é£æ ¼ï¼šç”¨æˆ·å†™å¾ªç¯é€»è¾‘
for _ in range(10):
    # 1. forward + backward + accumulation
    fb_response = requests.post(
        f"http://localhost:8000/v1/runs/{run_id}/forward_backward",
        json={
            "input_ids": batch["input_ids"].tolist()[0],
            "target_ids": batch["input_ids"].tolist()[0],
        }
    )
    job_id = fb_response.json()["job_id"]

    # 2. ç­‰å¾…æ¢¯åº¦ç´¯ç§¯å®Œæˆ
    # ... è¿™é‡Œå¯ä»¥å¼‚æ­¥ä¼˜åŒ– (polling / callback)

    # 3. optimizer step
    optim_response = requests.post(
        f"http://localhost:8000/v1/runs/{run_id}/optim_step",
        json={
            "learning_rate": 2e-4,
            "weight_decay": 0.01,
        }
    )

    print(f"Step: {_}, Loss: ...")
```

### 4. ç”Ÿæˆè¯„æµ‹

```python
# æ¨ç†é‡‡æ ·
response = requests.post(
    "http://localhost:8000/v1/sample",
    json={
        "prompt": "Once upon a time,",
        "max_new_tokens": 50,
        "temperature": 0.7,
    }
)
print(f"Generated: {response.json()}")
```

### 5. ä¿å­˜å’ŒåŠ è½½ checkpoint

```python
# ä¿å­˜
save_response = requests.post(
    f"http://localhost:8000/v1/runs/{run_id}/save",
    json={"name": "checkpoint_v1"}
)
print(save_response.json())

# æ–‡ä»¶ä¿å­˜åœ¨: checkpoints/{run_id}/checkpoint_v1.adapter.pt
#               checkpoints/{run_id}/checkpoint_v1.optimizer.pt
```

## CLI ä½¿ç”¨

```bash
# å¯åŠ¨æœåŠ¡å™¨
uv run eztinker server

# åˆ›å»ºè®­ç»ƒä¼šè¯
uv run eztinker create --model gpt2

# åˆ—å‡ºæ‰€æœ‰ä¼šè¯
uv run eztinker list

# ç”Ÿæˆ sample
uv run eztinker sample "Once upon a time," --max-tokens 50

# ä¿å­˜ checkpoint
uv run eztinker save my_run_id checkpoint_v1

# åˆ é™¤ä¼šè¯
uv run eztinker delete my_run_id
```

## Directory Structure

```
eztinker/
â”œâ”€â”€ src/eztinker/
â”‚   â”œâ”€â”€ api/          # FastAPI server
â”‚   â”œâ”€â”€ engine/       # Training & sampling engines
â”‚   â”œâ”€â”€ models/       # Pydantic models
â”‚   â”œâ”€â”€ core/         # State + checkpoint mgmt
â”‚   â””â”€â”€ cli/          # CLI (typer)
â”œâ”€â”€ checkpoints/      # Checkpoint files
â”œâ”€â”€ .env              # Environment config
â”œâ”€â”€ pyproject.toml    # uv project config
â””â”€â”€ README.md
```

## Environment Variables

```bash
# Set checkpoint directory (default: checkpoints)
export CHECKPOINTS_DIR=/path/to/checkpoints
export EZTINKER_BASE_URL=http://localhost:8000

# GPU support (CUDA)
export CUDA_VISIBLE_DEVICES=0
```

## å¼€å‘å·¥å…·é“¾ (uv + ruff + ty) ğŸš€

æˆ‘ä»¬ä½¿ç”¨**ç°ä»£ Python æœ€å¿«é€Ÿ**çš„å¼€å‘å·¥å…·é“¾ä»¥è·å¾—æœ€ä½³å¼€å‘ä½“éªŒï¼š

- **uv**: æé€ŸåŒ…ç®¡ç†å’Œé¡¹ç›®æ„å»º (Rust å®ç°ï¼Œ100x faster than pip)
- **ruff**: æé€Ÿçš„ Python linter å’Œæ ¼å¼åŒ–å™¨ (1000x faster than black + isort + autoflake + ...)
- **ty** (astral-sh/ty): æé€Ÿç±»å‹æ£€æŸ¥å™¨ (100x faster than mypy)

### é…ç½®æ–‡ä»¶

é¡¹ç›®å·²é…ç½®å®Œå–„çš„å·¥å…·é…ç½®æ–‡ä»¶ï¼š
- `.ruff.toml` - Ruff æ ¼å¼åŒ–å’Œ lint é…ç½®
- `.ty.toml` - Ty ç±»å‹æ£€æŸ¥é…ç½®
- `pyproject.toml` - é¡¹ç›®ä¾èµ–å’Œæ‰“åŒ…é…ç½®

### å¼€å‘å·¥ä½œæµ

```bash
# 1. æ ¼å¼åŒ–ä»£ç  (auto-format)
uv run ruff format src/

# 2. Lint æ£€æŸ¥ (static analysis)
uv run ruff check src/

# 3. ç±»å‹æ£€æŸ¥ (runtime correctness)
uv run ty check

# 4. è‡ªåŠ¨ä¿®å¤æ‰€æœ‰ lint é—®é¢˜
uv run ruff check src/ --fix

# 5. æ‰‹åŠ¨æµ‹è¯•
uv run eztinker server --reload

# 6. å®Œæ•´çš„è´¨é‡æ£€æŸ¥ (format + lint + type)
uv run ruff format src/ && uv run ruff check src/ && uv run ty check
```

### ç¯å¢ƒå˜é‡é…ç½®ç¤ºä¾‹

```bash
# .bashrc / .zshrc
# é…ç½®ä¾¿æ·çš„ shell alias
alias ezt-lint='uv run ruff check src/'
alias ezt-fmt='uv run ruff format src/'
alias ezt-type='uv run ty check'
alias ezt-qc='ezt-fmt && ezt-lint && ezt-type'
alias ezt-dev='uv run eztinker server --reload'
alias ezt-add='uv add'
alias ezt-rm='uv remove'
```

### å…¸å‹çš„ä¸Šæ‰‹æµç¨‹

```bash
# 1. æ·»åŠ æ–°ä¾èµ–
uv add <package-name>

# 2. æ ¼å¼åŒ–æ‰€æœ‰ä»£ç 
ezt-fmt

# 3. è¿è¡Œ linter
ezt-lint

# 4. ç±»å‹æ£€æŸ¥
ezt-type

# 5. å¯åŠ¨å¼€å‘æœåŠ¡å™¨
ezt-dev
```



## TODO (Future Enhancements)

- [ ] **LoRA Loading**: Load adapter from checkpoint `.adapter.pt` to sampler
- [ ] **Batch Training**: Batch processing for `forward_backward`
- [ ] **Multi-GPU**: Distributed training support
- [ ] **OpenAI Compatible API**: Inference interface support
- [ ] **Web UI Console**: Visualize training state
- [ ] **Scheduler**: Clock cycle scheduling like Tinker
- [ ] **Advanced Losses**: PPO/CISPO/DRO RL losses

## EZTinker vs Tinker

| Feature | EZTinker (MVP) | Tinker (Full) |
|---------|----------------|---------------|
| âœ… LoRA Fine-tuning | âœ… | âœ… |
| âœ… Checkpoint Management | âœ… | âœ… |
| âœ… Async/Future Pattern | âœ… | âœ… |
| âŒ Multi-GPU Worker Pool | âŒ | âœ… |
| âŒ Clock Cycle Scheduler | âŒ | âœ… |
| âŒ OpenAI Compatible | âŒ | âœ… |
| âŒ Custom Losses | âŒ | âœ… |

## License

MIT License - free to use, modify, distribute.# EZTinker
# EZTinker
